{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "31HNKSDIbU96",
        "outputId": "789aeacd-0b89-4250-defa-65e13fc01ccf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, types\n",
        "import pandas as pd\n",
        "from botocore.client import Config\n",
        "import ibm_boto3\n",
        "\n",
        "def __iter__(self): return 0\n",
        "\n",
        "# @hidden_cell\n",
        "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
        "# You might want to remove those credentials before you share the notebook.\n",
        "cos_client = ibm_boto3.client(service_name='s3',\n",
        "    ibm_api_key_id='WIb_ACjnfg57VW8XciXcPyyAXvbD1wVj0BCjSLJ4vB_X',\n",
        "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
        "    config=Config(signature_version='oauth'),\n",
        "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
        "\n",
        "bucket = 'imageclassification-donotdelete-pr-udm84rrbrf7tlw'\n",
        "object_key = 'TRAIN_SET.zip'\n",
        "\n",
        "streaming_body_3 = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\n",
        "\n",
        "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
        "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
        "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
        "# pandas documentation: http://pandas.pydata.org/"
      ],
      "metadata": {
        "id": "f1C3VYAsbjCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "import zipfile\n",
        "unzip=zipfile.ZipFile(BytesIO(streaming_body_3.read()),'r')\n",
        "file_paths=unzip.namelist()\n",
        "for path in file_paths:\n",
        "    unzip.extract(path)"
      ],
      "metadata": {
        "id": "At8gKaCjbrnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyk9HHOVbwG1",
        "outputId": "8c918717-78d0-4a12-e0de-5dc6439d5040"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np#used for numerical analysis\n",
        "import tensorflow #open source used for both ML and DL for computation\n",
        "from tensorflow.keras.models import Sequential #it is a plain stack of Layers\n",
        "from tensorflow.keras import layers #A Layer consists of a tensor-in tensor-out computation function\n",
        "#Dense Layer is the regular deeply connected neural network Layer\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "#Faltten-used fot flattening the input or change the dimension\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Dropout #Convolutional layer\n",
        "#MaxPooling2D-for downsampling the image\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "jegmy2oCb2Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "VCRTKd_Hb_zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_datagen.flow_from_directory(\n",
        "    r'/home/wsuser/work/TRAIN_SET',\n",
        "    target_size=(64, 64),batch_size=5,color_mode='rgb',class_mode='sparse')\n",
        "x_test = test_datagen.flow_from_directory(\n",
        "    r'/home/wsuser/work/TRAIN_SET',\n",
        "    target_size=(64, 64),batch_size=5,color_mode='rgb',class_mode='sparse') "
      ],
      "metadata": {
        "id": "8Ke6ugUFcA2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 2626 images belonging to 5 classes.\n",
        "Found 2626 images belonging to 5 classes."
      ],
      "metadata": {
        "id": "YNsJjUiIcENM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.class_indices)#checking the number of classes"
      ],
      "metadata": {
        "id": "mGo3MstvcHcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'APPLES': 0, 'BANANA': 1, 'ORANGE': 2, 'PINEAPPLE': 3, 'WATERMELON': 4}"
      ],
      "metadata": {
        "id": "Rq5dF_-qcQSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter as c \n",
        "c(x_train .labels)"
      ],
      "metadata": {
        "id": "ZgmrPS8pcNMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counter({0: 606, 1: 445, 2: 479, 3: 621, 4: 475})"
      ],
      "metadata": {
        "id": "8tcmJSWhcRIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the CNN classifier = Sequential()\n",
        "classifier = Sequential()\n",
        "# First convolution layer and pooling\n",
        "classifier.add(Conv2D(32,(3, 3), input_shape=(64, 64, 3),activation='relu')) \n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Second convolution layer and pooling\n",
        "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# input_shape is going to be the pooled feature maps from the previous convolution layer \n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Flattening the Layers\n",
        "classifier.add(Flatten())\n",
        "# Adding fully connected Layer a\n",
        "classifier.add(Dense (units=128, activation='relu'))\n",
        "classifier.add(Dense (units=5, activation='softmax')) # softmax for more than 2"
      ],
      "metadata": {
        "id": "F49iS23Ncml7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.summary()"
      ],
      "metadata": {
        "id": "8H4NxAYbcrgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: \"sequential_3\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " conv2d_2 (Conv2D)           (None, 62, 62, 32)        896       \n",
        "                                                                 \n",
        " max_pooling2d_2 (MaxPooling  (None, 31, 31, 32)       0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " conv2d_3 (Conv2D)           (None, 29, 29, 32)        9248      \n",
        "                                                                 \n",
        " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " flatten_1 (Flatten)         (None, 6272)              0         \n",
        "                                                                 \n",
        " dense_2 (Dense)             (None, 128)               802944    \n",
        "                                                                 \n",
        " dense_3 (Dense)             (None, 5)                 645       \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 813,733\n",
        "Trainable params: 813,733\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________"
      ],
      "metadata": {
        "id": "FDE2v9Y5c8FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i2PfofcOdBaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Fitting the model\n",
        "classifier.fit_generator(\n",
        "    generator=x_train,steps_per_epoch = len(x_train), \n",
        "    epochs=20,validation_data=x_test,validation_steps = len(x_test)) # No of images in test set"
      ],
      "metadata": {
        "id": "Mcp1_9kYdFR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/20\n",
        "526/526 [==============================] - 15s 27ms/step - loss: 0.1318 - accuracy: 0.9524 - val_loss: 1.6592e-04 - val_accuracy: 1.0000\n",
        "Epoch 2/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 0.0206 - accuracy: 0.9950 - val_loss: 0.4374 - val_accuracy: 0.7871\n",
        "Epoch 3/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 0.0325 - accuracy: 0.9897 - val_loss: 8.4484e-05 - val_accuracy: 1.0000\n",
        "Epoch 4/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 1.2242e-04 - accuracy: 1.0000 - val_loss: 1.7502e-05 - val_accuracy: 1.0000\n",
        "Epoch 5/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 3.8190e-05 - accuracy: 1.0000 - val_loss: 1.0376e-05 - val_accuracy: 1.0000\n",
        "Epoch 6/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 2.1320e-05 - accuracy: 1.0000 - val_loss: 4.5576e-06 - val_accuracy: 1.0000\n",
        "Epoch 7/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 8.8509e-06 - accuracy: 1.0000 - val_loss: 2.7205e-06 - val_accuracy: 1.0000\n",
        "Epoch 8/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 7.3976e-06 - accuracy: 1.0000 - val_loss: 1.5667e-06 - val_accuracy: 1.0000\n",
        "Epoch 9/20\n",
        "526/526 [==============================] - 14s 26ms/step - loss: 5.0094e-06 - accuracy: 1.0000 - val_loss: 9.0818e-07 - val_accuracy: 1.0000\n",
        "Epoch 10/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 2.3061e-06 - accuracy: 1.0000 - val_loss: 6.9995e-07 - val_accuracy: 1.0000\n",
        "Epoch 11/20\n",
        "526/526 [==============================] - 14s 26ms/step - loss: 2.4215e-06 - accuracy: 1.0000 - val_loss: 5.5605e-07 - val_accuracy: 1.0000\n",
        "Epoch 12/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 1.2473e-06 - accuracy: 1.0000 - val_loss: 4.2676e-07 - val_accuracy: 1.0000\n",
        "Epoch 13/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 8.6773e-07 - accuracy: 1.0000 - val_loss: 3.2626e-07 - val_accuracy: 1.0000\n",
        "Epoch 14/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 2.0117e-06 - accuracy: 1.0000 - val_loss: 1.9171e-07 - val_accuracy: 1.0000\n",
        "Epoch 15/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 4.7506e-07 - accuracy: 1.0000 - val_loss: 1.0427e-07 - val_accuracy: 1.0000\n",
        "Epoch 16/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 3.3093e-07 - accuracy: 1.0000 - val_loss: 7.6310e-08 - val_accuracy: 1.0000\n",
        "Epoch 17/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 2.6706e-07 - accuracy: 1.0000 - val_loss: 4.1174e-08 - val_accuracy: 1.0000\n",
        "Epoch 18/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 2.3379e-07 - accuracy: 1.0000 - val_loss: 4.5804e-08 - val_accuracy: 1.0000\n",
        "Epoch 19/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 2.2811e-07 - accuracy: 1.0000 - val_loss: 2.8781e-08 - val_accuracy: 1.0000\n",
        "Epoch 20/20\n",
        "526/526 [==============================] - 14s 27ms/step - loss: 1.1122e-07 - accuracy: 1.0000 - val_loss: 1.3437e-08 - val_accuracy: 1.0000"
      ],
      "metadata": {
        "id": "F-H7uVpvdIhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJNMtMgEdYck",
        "outputId": "a7d905a2-3293-4db8-ef0e-213a46c3dd4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.save('nutrition.h5')"
      ],
      "metadata": {
        "id": "zWgn-brvdjAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "-WnDDijMdpw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset/  nutrition.h5  TRAIN_SET/"
      ],
      "metadata": {
        "id": "iyNQaxtRdn_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pbeRO9FZeVB3",
        "outputId": "f991b2ac-0dba-482c-cd94-b3e4e4a4b905"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Predicting our results\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model = load_model(\"nutrition.h5\") #Loading the model for testing\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "4fdJz_r7eeSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img =image.load_img(r\"/home/wsuser/work/TRAIN_SET/ORANGE/100_100.jpg\",grayscale=False, target_size= (64,64))\n",
        "x = image.img_to_array(img)#image to array\n",
        "x = np.expand_dims(x,axis=0) #changing the shape =\n",
        "y =np.argmax(model.predict(x),axis=1)\n",
        "index=['APPLES', 'BANANA', 'ORANGE','PINEAPPLE','WATERMELON']\n",
        "index[y[0]]"
      ],
      "metadata": {
        "id": "d2uA4BADejVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'ORANGE'"
      ],
      "metadata": {
        "id": "wWwKF8UCen8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IBM Deployment"
      ],
      "metadata": {
        "id": "omv_8ad5eq8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watson-machine-learning-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUVUlIfte0PQ",
        "outputId": "1f5659d3-0f6c-4e52-8e23-b072c18e5598"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting watson-machine-learning-client\n",
            "  Downloading watson_machine_learning_client-1.0.391-py3-none-any.whl (538 kB)\n",
            "\u001b[K     |████████████████████████████████| 538 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (0.8.10)\n",
            "Collecting lomond\n",
            "  Downloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (2022.9.24)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (1.3.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (1.24.3)\n",
            "Collecting ibm-cos-sdk\n",
            "  Downloading ibm-cos-sdk-2.12.0.tar.gz (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.13-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (4.64.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.13\n",
            "  Downloading botocore-1.29.13-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.13->boto3->watson-machine-learning-client) (2.8.2)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 67.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.13->boto3->watson-machine-learning-client) (1.15.0)\n",
            "Collecting ibm-cos-sdk-core==2.12.0\n",
            "  Downloading ibm-cos-sdk-core-2.12.0.tar.gz (956 kB)\n",
            "\u001b[K     |████████████████████████████████| 956 kB 55.1 MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.12.0\n",
            "  Downloading ibm-cos-sdk-s3transfer-2.12.0.tar.gz (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 70.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->watson-machine-learning-client) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->watson-machine-learning-client) (2.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->watson-machine-learning-client) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->watson-machine-learning-client) (1.19.5)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.12.0-py3-none-any.whl size=73931 sha256=32501973f32267aa4bcad03662d33887ab744b1750f81bcbb2e2c19f9420e66f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/94/29/2b57327cf00664b6614304f7958abd29d77ea0e5bbece2ea57\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.12.0-py3-none-any.whl size=562962 sha256=31e5319d44b480105eee37726c1fc91097e481539eb870d60f0cf2540a9e9055\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/56/fb/5cd6f4f40406c828a5289b95b2752a4d142a9afb359244ed8d\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.12.0-py3-none-any.whl size=89778 sha256=8a342df44ac084e0cfeee72c66cbec6fb3d0b4beb84ea2198933d7188a25c716\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/79/6a/ffe3370ed7ebc00604f9f76766e1e0348dcdcad2b2e32df9e1\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: urllib3, requests, jmespath, ibm-cos-sdk-core, botocore, s3transfer, ibm-cos-sdk-s3transfer, lomond, ibm-cos-sdk, boto3, watson-machine-learning-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigquery 3.3.6 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.26.13 botocore-1.29.13 ibm-cos-sdk-2.12.0 ibm-cos-sdk-core-2.12.0 ibm-cos-sdk-s3transfer-2.12.0 jmespath-0.10.0 lomond-0.3.3 requests-2.28.1 s3transfer-0.6.0 urllib3-1.26.12 watson-machine-learning-client-1.0.391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.2.4\n",
        "!pip install tensorflow==2.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4GZv-mKFko2Q",
        "outputId": "4fc8e7eb-1e94-40ab-a3a4-b32575315f9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 31.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (6.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.2.4 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.5.0\n",
            "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 19 kB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 49.6 MB/s \n",
            "\u001b[?25hCollecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 71.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 82.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.9.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.19.6)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.38.3)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.2)\n",
            "Building wheels for collected packages: termcolor, wrapt\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=5ee4ad08221707a0bfb183df8deb279def35f00cdbbfb7fd20b7c06c36fd600a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68721 sha256=b7da7b12606df8930971da5c5fb2329dacedf85177958e6e1752fac20bb40703\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built termcolor wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, keras-nightly, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.50.0\n",
            "    Uninstalling grpcio-1.50.0:\n",
            "      Successfully uninstalled grpcio-1.50.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.3.0\n",
            "    Uninstalling absl-py-1.3.0:\n",
            "      Successfully uninstalled absl-py-1.3.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.1.0\n",
            "    Uninstalling termcolor-2.1.0:\n",
            "      Successfully uninstalled termcolor-2.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "pydantic 1.10.2 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 numpy-1.19.5 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson_machine_learning import APIClient\n",
        "wml_credentials={\n",
        "    \"url\":\"https://us-south.ml.cloud.ibm.com\",\n",
        "    \"apikey\": \"mvD9uH2XT7dZZf-7SAS7hLPBA8AfBd9pCSzGNIsC26uO\"\n",
        "}\n",
        "\n",
        "client=APIClient(wml_credentials)"
      ],
      "metadata": {
        "id": "EXhWkVfCoOlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client"
      ],
      "metadata": {
        "id": "MrfMrQWOoQC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-lTJhtjzoZ5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def guid_space_name(client,nutrition_deploy):\n",
        "    space=client.spaces.get_details()\n",
        "    return(next(item for item in space['resources'] if item['entity']['name']==nutrition_deploy)['metadata']['id'])"
      ],
      "metadata": {
        "id": "7qP2ZMFuoaoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment"
      ],
      "metadata": {
        "id": "T4NicNaMoeq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "space_uid=guid_space_name(client,'nutrition_deploy')\n",
        "print(\"Space UID \" + space_uid)"
      ],
      "metadata": {
        "id": "oYE2uO_AoiQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Space UID ef027b06-f8be-4794-a8d8-78f31828274d"
      ],
      "metadata": {
        "id": "nDsgkd6Hongw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.set.default_space(space_uid)"
      ],
      "metadata": {
        "id": "w9XqwMAiosJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'SUCCESS'"
      ],
      "metadata": {
        "id": "esZtMGqdpAma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.software_specifications.list(200)"
      ],
      "metadata": {
        "id": "CsOM8FK3ovfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "software_space_uid=client.software_specifications.get_uid_by_name('tensorflow_rt22.1-py3.9')"
      ],
      "metadata": {
        "id": "EhHs18GppzDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "software_space_uid"
      ],
      "metadata": {
        "id": "YogukLHep0SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'acd9c798-6974-5d2f-a657-ce06e986df4d'"
      ],
      "metadata": {
        "id": "BBXnXRcZqOEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf nutrition-classification-model.tgz nutrition.h5"
      ],
      "metadata": {
        "id": "d_c8NVDTqD5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nutrition.h5"
      ],
      "metadata": {
        "id": "ExDSaszXqccg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_details=client.repository.store_model(model ='nutrition-classification-model.tgz', meta_props={\n",
        "    client.repository.ModelMetaNames.NAME: \"CNN Model Building\",\n",
        "    client.repository.ModelMetaNames.TYPE: 'tensorflow_2.7',\n",
        "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_space_uid\n",
        "})"
      ],
      "metadata": {
        "id": "_TjUGr9wqgRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=client.repository.get_model_id(model_details)"
      ],
      "metadata": {
        "id": "aSJD2xgJqk2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id"
      ],
      "metadata": {
        "id": "XeWVM3cRqvR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'cfd303f2-c0bc-44b4-b58b-52def6a33906'"
      ],
      "metadata": {
        "id": "fz4lAsZLrHTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "-Umv03f-rAok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset/  nutrition-classification-model.tgz  nutrition.h5  TRAIN_SET/"
      ],
      "metadata": {
        "id": "9RY9IXj4rU8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.repository.download(model_id,'nutrition.tar.gb')"
      ],
      "metadata": {
        "id": "ZDEgwOOwraXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successfully saved model content to file: 'nutrition.tar.gb'\n",
        "'/home/wsuser/work/nutrition.tar.gb'"
      ],
      "metadata": {
        "id": "dpGIfYvyrkhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "YaU0806Xrlz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'/home/wsuser/work'"
      ],
      "metadata": {
        "id": "MYApKZX1rrqB"
      }
    }
  ]
}